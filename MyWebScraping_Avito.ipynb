{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "url = 'https://www.avito.ma/fr/maroc/%C3%A0_vendre?o='\n",
        "page = requests.get(url, headers=headers)\n",
        "print(page)"
      ],
      "metadata": {
        "id": "X8D4gAQyAdg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install js2xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jumFjOKRyr5U",
        "outputId": "4c5c7e7b-94f2-46b8-a250-f88c9f44906c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting js2xml\n",
            "  Downloading js2xml-0.5.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Collecting calmjs.parse (from js2xml)\n",
            "  Downloading calmjs.parse-1.3.1-py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.0/297.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from js2xml) (4.9.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from calmjs.parse->js2xml) (67.7.2)\n",
            "Collecting ply>=3.6 (from calmjs.parse->js2xml)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ply, calmjs.parse, js2xml\n",
            "Successfully installed calmjs.parse-1.3.1 js2xml-0.5.0 ply-3.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from js2xml import parse\n",
        "import pandas as pd\n",
        "\n",
        "# Lists to store the scraped data in\n",
        "addressLocality = []\n",
        "addressRegion = []\n",
        "categorys = []\n",
        "product_id = []\n",
        "telephones = []\n",
        "publicationType = []\n",
        "names = []\n",
        "prices = []\n",
        "pages = [str(i) for i in range(2, 10000)]\n",
        "\n",
        "for page in pages:\n",
        "    url = \"https://www.avito.ma/fr/maroc/%C3%A0_vendre?o=\" + page\n",
        "    session = requests.Session()\n",
        "    adapter = requests.adapters.HTTPAdapter(max_retries=20)\n",
        "    session.mount('https://', adapter)\n",
        "    session.mount('http://', adapter)\n",
        "    pg = session.get(url)\n",
        "\n",
        "    if pg.status_code != 200:\n",
        "        print(f\"Failed to fetch page {page}. Status code: {pg.status_code}\")\n",
        "        continue\n",
        "\n",
        "    html_soup = BeautifulSoup(pg.text, 'html.parser')\n",
        "\n",
        "    # Extracting the url of each product :\n",
        "    items_list = html_soup.find(class_=\"listing listing-thumbs\").findAll(\"div\", class_=\"item-info ctext1 mls\")\n",
        "    items_urls = [i.find('a', href=True)['href'] for i in items_list]\n",
        "\n",
        "    for url in items_urls:\n",
        "        session = requests.Session()\n",
        "        adapter = requests.adapters.HTTPAdapter(max_retries=20)\n",
        "        session.mount('https://', adapter)\n",
        "        session.mount('http://', adapter)\n",
        "        response = session.get(url)\n",
        "\n",
        "        data = {}\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch data from URL: {url}. Status code: {response.status_code}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            html_soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            price = html_soup.find(\"div\", class_=\"panel-body\").span.string\n",
        "            script = html_soup.find(\"div\", class_=\"container mbm\").find_all(\"script\", {\"type\": \"text/javascript\"})[-1]\n",
        "            price = html_soup.find(\"div\", class_=\"panel-body\").span.string\n",
        "            js_code = script.string\n",
        "            js_tree = parse(js_code)\n",
        "\n",
        "            for assign_node in js_tree.xpath('//assign'):\n",
        "                key = assign_node.find('left/identifier').get('name')\n",
        "                value = assign_node.find('right').get('value')\n",
        "                data[key] = value\n",
        "\n",
        "            # If the data is not empty, then extract:\n",
        "            if data:\n",
        "                prices.append(price)  # the price\n",
        "                addressLocality.append(data.get(\"addressLocality\", ''))  # Local address\n",
        "                addressRegion.append(data.get(\"addressRegion\", ''))  # Region address\n",
        "                categorys.append(data.get(\"category\", ''))  # category\n",
        "                telephones.append(data.get(\"telephone\", ''))  # telephones\n",
        "                publicationType.append(data.get(\"publisherType\", ''))  # publication type\n",
        "                names.append(data.get(\"name\", ''))  # the product name\n",
        "                product_id.append(data.get(\"id\", ''))  # product_id\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error while processing URL: {url}. Exception: {e}\")\n",
        "\n",
        "# Save our data in an excel file:\n",
        "dataset = pd.DataFrame({\n",
        "    \"Product_name\": names,\n",
        "    \"Product_id\": product_id,\n",
        "    \"Product_Category\": categorys,\n",
        "    \"price\": prices,\n",
        "    \"Phone_number\": telephones,\n",
        "    \"Professional_Publication\": publicationType,\n",
        "    \"Region_address\": addressRegion,\n",
        "    \"Local_address\": addressLocality\n",
        "})\n",
        "dataset.to_csv(\"MyAvitoDataset.csv\")"
      ],
      "metadata": {
        "id": "3H7yiMNjQBNH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b8SzJtvpyelK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}